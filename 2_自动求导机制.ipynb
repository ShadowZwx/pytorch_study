{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对标量进行自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([2]), requires_grad = True)\n",
    "y = x + 2\n",
    "z = y ** 2 + 3\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更复杂的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0334,  0.0303,  0.0692, -0.0453, -0.0485,  0.0187, -0.0848,  0.0009,\n",
      "          0.0673,  0.0100],\n",
      "        [-0.0334,  0.0303,  0.0692, -0.0453, -0.0485,  0.0187, -0.0848,  0.0009,\n",
      "          0.0673,  0.0100],\n",
      "        [-0.0334,  0.0303,  0.0692, -0.0453, -0.0485,  0.0187, -0.0848,  0.0009,\n",
      "          0.0673,  0.0100],\n",
      "        [-0.0334,  0.0303,  0.0692, -0.0453, -0.0485,  0.0187, -0.0848,  0.0009,\n",
      "          0.0673,  0.0100],\n",
      "        [-0.0334,  0.0303,  0.0692, -0.0453, -0.0485,  0.0187, -0.0848,  0.0009,\n",
      "          0.0673,  0.0100],\n",
      "        [-0.0334,  0.0303,  0.0692, -0.0453, -0.0485,  0.0187, -0.0848,  0.0009,\n",
      "          0.0673,  0.0100],\n",
      "        [-0.0334,  0.0303,  0.0692, -0.0453, -0.0485,  0.0187, -0.0848,  0.0009,\n",
      "          0.0673,  0.0100],\n",
      "        [-0.0334,  0.0303,  0.0692, -0.0453, -0.0485,  0.0187, -0.0848,  0.0009,\n",
      "          0.0673,  0.0100],\n",
      "        [-0.0334,  0.0303,  0.0692, -0.0453, -0.0485,  0.0187, -0.0848,  0.0009,\n",
      "          0.0673,  0.0100],\n",
      "        [-0.0334,  0.0303,  0.0692, -0.0453, -0.0485,  0.0187, -0.0848,  0.0009,\n",
      "          0.0673,  0.0100]])\n",
      "tensor([[0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200]])\n",
      "tensor([[ 0.0582,  0.0582,  0.0582,  0.0582,  0.0582],\n",
      "        [ 0.0369,  0.0369,  0.0369,  0.0369,  0.0369],\n",
      "        [ 0.0373,  0.0373,  0.0373,  0.0373,  0.0373],\n",
      "        [ 0.1022,  0.1022,  0.1022,  0.1022,  0.1022],\n",
      "        [-0.0421, -0.0421, -0.0421, -0.0421, -0.0421],\n",
      "        [-0.0071, -0.0071, -0.0071, -0.0071, -0.0071],\n",
      "        [ 0.0535,  0.0535,  0.0535,  0.0535,  0.0535],\n",
      "        [ 0.0073,  0.0073,  0.0073,  0.0073,  0.0073],\n",
      "        [-0.0216, -0.0216, -0.0216, -0.0216, -0.0216],\n",
      "        [-0.0706, -0.0706, -0.0706, -0.0706, -0.0706]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.randn(10, 10), requires_grad=True)\n",
    "y = Variable(torch.randn(10, 5), requires_grad=True)\n",
    "w = Variable(torch.randn(10, 5), requires_grad=True)\n",
    "out = torch.mean(y - torch.matmul(x, w)) # torch.matmul 矩阵乘法\n",
    "out.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 复杂情况的自动求导\n",
    "对多维数组的自动求导机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 3.]], requires_grad=True)\n",
      "tensor([[0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "m = Variable(torch.FloatTensor([[2, 3]]), requires_grad=True) # 构建一个 1 x 2 的矩阵 数据分别是 2 ，3\n",
    "n = Variable(torch.zeros(1, 2)) # 构建一个相同大小的 0 矩阵\n",
    "print(m)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4., 27.]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "# 通过 m 中的值计算新的 n 中的值\n",
    "n[0, 0] = m[0, 0] ** 2\n",
    "n[0, 1] = m[0, 1] ** 3\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 PyTorch 中，多维如果要调用自动求导，需要往backward()中传入一个参数，这个参数的形状和 n 一样大，比如是 $(w_0,\\ w_1)$，那么自动求导的结果就是：\n",
    "$$\n",
    "\\frac{\\partial n}{\\partial m_0} = w_0 \\frac{\\partial n_0}{\\partial m_0} + w_1 \\frac{\\partial n_1}{\\partial m_0}\n",
    "$$$$\n",
    "\\frac{\\partial n}{\\partial m_1} = w_0 \\frac{\\partial n_0}{\\partial m_1} + w_1 \\frac{\\partial n_1}{\\partial m_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4., 27.]])\n"
     ]
    }
   ],
   "source": [
    "n.backward(torch.ones_like(n))\n",
    "print(m.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多次自动求导\n",
    "调用 backward 我们可以进行一次自动求导，如果我们再调用一次 backward，程序会报错，没有办法再做一次。这是因为 PyTorch 默认做完一次自动求导之后，计算图就被丢弃。所以两次自动求导需要手动设置。eg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.FloatTensor([3]), requires_grad=True)\n",
    "y = x * 2 + x ** 2 + 3\n",
    "y.backward(retain_graph = True) # 设置 retain_graph 为 True 来保留计算图\n",
    "print(x.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([16.])\n"
     ]
    }
   ],
   "source": [
    "y.backward() # 再做一次自动求导，这次不保留计算图\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x 的梯度变成了 16，因为这里做了两次自动求导，所以第一次的梯度 8 和第二次的梯度 8 加起来得到了 16 的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小练习\n",
    "\n",
    "定义\n",
    "\n",
    "$$\n",
    "x = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "x_0 \\\\\n",
    "x_1\n",
    "\\end{matrix}\n",
    "\\right] = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "2 \\\\\n",
    "3\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$$$\n",
    "k = (k_0,\\ k_1) = (x_0^2 + 3 x_1,\\ 2 x_0 + x_1^2)\n",
    "$$\n",
    "我们希望求得\n",
    "\n",
    "$$\n",
    "j = \\left[\n",
    "\\begin{matrix}\n",
    "\\frac{\\partial k_0}{\\partial x_0} & \\frac{\\partial k_0}{\\partial x_1} \\\\\n",
    "\\frac{\\partial k_1}{\\partial x_0} & \\frac{\\partial k_1}{\\partial x_1}\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "参考答案：\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "4 & 3 \\\\\n",
    "2 & 6 \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13., 13.], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.FloatTensor([2,3]),requires_grad = True )\n",
    "k = Variable(torch.zeros(2))\n",
    "k[0] = x[0] ** 2 + x[1] * 3 \n",
    "k[1] = x[1] ** 2 + x[0] * 2 \n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 3.],\n",
      "        [2., 6.]])\n"
     ]
    }
   ],
   "source": [
    "j = torch.zeros(2,2)\n",
    "k.backward(torch.FloatTensor([1, 0]), retain_graph=True)\n",
    "j[0] = x.grad\n",
    "\n",
    "x.grad.data.zero_() # 归零之前求得的梯度\n",
    "\n",
    "k.backward(torch.FloatTensor([0, 1]))\n",
    "j[1] = x.grad\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
