{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多分类问题\n",
    "对于多分类问题而言，我们的 loss 函数使用一个更加复杂的函数，叫交叉熵\n",
    "### softmax\n",
    "softmax 函数示例如下\n",
    "\n",
    "![null](https://camo.githubusercontent.com/262cf3339ba6413b35650c9136f19d34761b366a/68747470733a2f2f7773342e73696e61696d672e636e2f6c617267652f303036744b6654636c7931666d6c78746e666d34666a33306c6c30626e7133632e6a7067)\n",
    "\n",
    "对于网络的输出 $z_1, z_2, \\cdots z_k$，我们首先对他们每个都取指数变成 $e^{z_1}, e^{z_2}, \\cdots, e^{z_k}$，那么每一项都除以他们的求和，也就是\n",
    "\n",
    "$$\n",
    "z_i \\rightarrow \\frac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}}\n",
    "$$\n",
    "如果对经过 softmax 函数的所有项求和就等于 1，所以他们每一项都分别表示属于其中某一类的概率\n",
    "\n",
    "## 交叉熵\n",
    "交叉熵衡量两个分布相似性的一种度量方式，前面讲的二分类问题的 loss 函数就是交叉熵的一种特殊情况，交叉熵的一般公式为\n",
    "\n",
    "$$\n",
    "cross\\_entropy(p, q) = E_{p}[-\\log q] = - \\frac{1}{m} \\sum_{x} p(x) \\log q(x)\n",
    "$$\n",
    "对于二分类问题我们可以写成\n",
    "\n",
    "$$\n",
    "-\\frac{1}{m} \\sum_{i=1}^m (y^{i} \\log sigmoid(x^{i}) + (1 - y^{i}) \\log (1 - sigmoid(x^{i}))\n",
    "$$\n",
    "这就是我们之前讲的二分类问题的 loss，当时我们并没有解释原因，只是给出了公式，然后解释了其合理性，现在我们给出了公式去证明这样取 loss 函数是合理的\n",
    "\n",
    "交叉熵是信息理论里面的内容，这里不再具体展开，更多的内容，可以看到下面的链接\n",
    "https://blog.csdn.net/rtygbwwwerr/article/details/50778098\n",
    "https://blog.csdn.net/wenzishou/article/details/77618992"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
